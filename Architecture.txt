Architecture Design Quick Summary: Final Project 

Setup Assumptions, Running, and File Dependencies:

The starting assumptions is the user has logged into AWS and has the appropriate ports/system configuration for hadoop, hive, and ports open for Tableau. Additionally it is assumed that hadoop and postgres are both installed already in a manner consistent with out previous labs. Please note that file paths may be different if the user changes the installation paths. 

Architecture Description:

This application is based off leveraging hdfs to store three primary datasets including nonprofit IRS 990 tax information, state population data, and state gdp data. Hive is then used to query and retrieve data relevent to specific questions and Tableau in connection to the hive server can be leveraged for analysis. 

Application Idea:

This  application is focused on analysis of the nonprofit organizations focusing on geographic trends. The application stores and organizes data using a defined schema and alloes for query and analysis.  This allows for exploration of geographic locations and limitations for NGOs striving for success.  

Directory Descriptions:

The Github repository for this Final Projet has the following folders:

Datasets - this was the initial folder to share and organize the data
Milestones - Includes copies of all of the milestones for the proejct
Scripts - Includes any scipts that were used
Twitter - includes the start of the files for the streaing twitter aspect of the project 


Twitter Application (not complete)

Initially we intended to add in streaming twitter data on the most successful NGOs to see identify possible messaging wording related to areas of interest. For instance if these NGOs are using social media to advertise events or recruit volunteers. However due to time constraints we were unable to impliment this fully. The twitter folder on github has some of the files we started working on. The specific NGOs or hashtags can be modified in the tweets.py file. In this file, we currently have just general hashtags related to NGOs since we didn't get to finish this analysis. 

The twitter work assumes that ipython2.7 is being used and streamparse, lein, psycopg2, tweepy, and postgres are all set up and running appropriately. Additionally tables are created in postgres to store the word count data. The files in the twitter folder then are replaced similar to the example below. 

*******************
	$sparse quickstart Tweetwordcount

	`move the files pulled from Github to the correct file locations in Tweetwordcount (bolts, spout, topoligies)
	$cp githubpath/tweetwordcount.clj  /path/Tweetwordcount/topologies/tweetwordcount.clj 
	$rm /path/Tweetwordcount/topologies/wordcount.clj
	$cp githubpath/tweets.py /path/Tweetwordcount/src/spouts/tweets.py
	$cp githubpath/parse.py  /path/Tweetwordcount/src/bolts/parse.py 
	$cp -f githubpath/wordcount.py  /path/Tweetwordcount/src/bolts/wordcount.py


	`change to the tweetwordcount directory
	cd Tweetwordcount

	`run the application
	sparse run
**********************

